What would it mean to generate images from the sound of speech, not from the semantics of the words themselves?

I built a visualizer for human speech with a generative model called a Variational Auto Encoder (VAE), for <i>Fundamentals of Speech Recognition</i> (E6998), a graduate level speech recognition class at Columbia. The project was inspired by <a href="https://github.com/alex-calderwood/phonaesthesia/blob/master/papers/leiberman_paper.pdf">this incredible paper by Zach Leiberman.</a>

I want to return to this project one day. It was really fun to work on and gave me a bit more experience hand-coding generative deep neural networks. Got to learn about the VAE, which requires some fascinating stats -- sampling from latent distributions that you've forced to be Gaussian.